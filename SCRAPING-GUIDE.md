# üîç Guide du Scraping

Guide pour tester et configurer le scraping des conventions Supernatural.

## üöÄ Tester le scraping en local

### Commande rapide

```bash
npm run scrape
```

Cette commande :
1. Installe les d√©pendances du scraper (axios, cheerio)
2. Ex√©cute le scraping sur tous les sites configur√©s
3. G√©n√®re/met √† jour `data/conventions.json`

### Voir les r√©sultats d√©taill√©s

```bash
npm run scrape:test
```

Affiche le JSON complet apr√®s le scraping.

## üìä R√©sultat attendu

Lors de l'ex√©cution, vous verrez :

```
üî• Supernatural Conventions Scraper üî•

Starting scraping process...

üîç Scraping Creation Entertainment...
‚úÖ Found X conventions from Creation Entertainment

üîç Scraping Starfury Conventions...
‚úÖ Starfury: X total conventions

üîç Scraping HonCon...
‚úÖ HonCon: X total conventions

üîç Searching Eventbrite...
‚úÖ Eventbrite: X total conventions

üîç Scraping People Conventions...
‚úÖ People Conventions: X total conventions

‚úÖ Saved X conventions to data/conventions.json

üéâ Scraping completed!
üìä Total conventions found: X
```

## ‚ö†Ô∏è Erreurs courantes

### Erreur 404 ou ENOTFOUND

```
‚ùå Error scraping [Site]: Request failed with status code 404
‚ùå Error scraping [Site]: getaddrinfo ENOTFOUND www.example.com
```

**Causes :**
- Le site a chang√© d'URL
- Le site bloque les scrapers
- Le site n'existe plus
- Protection anti-bot

**Solutions :**
1. V√©rifier que l'URL existe dans un navigateur
2. Adapter les s√©lecteurs CSS dans `scraper/scrape.js`
3. Ajouter des headers plus r√©alistes
4. Utiliser les conventions manuelles (voir ci-dessous)

### Erreur 405 (Method Not Allowed)

```
‚ùå Error scraping Eventbrite: Request failed with status code 405
```

Le site bloque les requ√™tes GET simples. Il faut :
- Utiliser un vrai navigateur (Puppeteer)
- Ou ajouter les conventions manuellement

## ‚úçÔ∏è Ajouter des conventions manuellement

√âditer `scraper/scrape.js`, fonction `addManualConventions()` :

```javascript
function addManualConventions() {
  const manualConventions = [
    {
      id: 'unique-id-2025',                    // ID unique (slug)
      name: 'Nom de la Convention',            // Nom complet
      location: 'Ville, Pays',                 // Lieu
      date: 'Mois JJ-JJ, YYYY',               // Date format√©e
      url: 'https://site.com',                 // URL de la convention
      source: 'Manual Entry',                  // Source (ne pas changer)
      guests: ['Acteur 1', 'Acteur 2']        // Liste des invit√©s
    },
    // Ajouter d'autres conventions ici...
  ];

  conventions.push(...manualConventions);
}
```

**Exemple r√©el :**

```javascript
{
  id: 'honcon-2025-germany',
  name: 'HonCon 2025',
  location: 'D√ºsseldorf, Germany',
  date: 'June 20-22, 2025',
  url: 'https://www.honcon.de',
  source: 'Manual Entry',
  guests: ['Jensen Ackles', 'Jared Padalecki', 'Misha Collins']
}
```

Apr√®s modification, relancer :

```bash
npm run scrape
```

## üõ†Ô∏è Am√©liorer le scraping

### 1. Adapter les s√©lecteurs CSS

Chaque site a une structure HTML diff√©rente. Pour adapter :

1. Ouvrir le site dans le navigateur
2. **F12** ‚Üí Inspecteur
3. Trouver les √©l√©ments contenant les conventions
4. Noter les classes CSS et balises
5. Modifier dans `scraper/scrape.js`

**Exemple :**

```javascript
async function scrapeMonSite() {
  try {
    const response = await axios.get('https://monsite.com/events');
    const $ = cheerio.load(response.data);

    // Adapter ces s√©lecteurs selon le site
    $('.event-card').each((i, elem) => {
      const name = $(elem).find('.event-title').text().trim();
      const location = $(elem).find('.event-location').text().trim();
      const date = $(elem).find('.event-date').text().trim();
      const url = $(elem).find('a').attr('href');

      // Filtrer Supernatural
      if (name.toLowerCase().includes('supernatural')) {
        conventions.push({
          id: generateId(name, date),
          name,
          location,
          date,
          url: url.startsWith('http') ? url : `https://monsite.com${url}`,
          source: 'Mon Site',
          guests: []
        });
      }
    });
  } catch (error) {
    console.error('‚ùå Error:', error.message);
  }
}
```

### 2. Ajouter des headers r√©alistes

Certains sites bloquent les bots. Ajouter :

```javascript
const response = await axios.get(url, {
  timeout: 10000,
  headers: {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Referer': 'https://www.google.com/'
  }
});
```

### 3. Utiliser Puppeteer pour les sites JavaScript

Pour les sites qui n√©cessitent JavaScript (comme Eventbrite) :

```bash
cd scraper
npm install puppeteer
```

```javascript
const puppeteer = require('puppeteer');

async function scrapeWithBrowser() {
  const browser = await puppeteer.launch({ headless: true });
  const page = await browser.newPage();
  await page.goto('https://site.com');

  const content = await page.content();
  const $ = cheerio.load(content);

  // Scraper normalement avec cheerio

  await browser.close();
}
```

## üìù Fichier de sortie

Le scraper g√©n√®re `data/conventions.json` avec ce format :

```json
{
  "lastUpdate": "2025-12-07T16:00:00.000Z",
  "count": 3,
  "conventions": [
    {
      "id": "convention-id",
      "name": "Nom de la Convention",
      "location": "Ville, Pays",
      "date": "Mois JJ-JJ, YYYY",
      "url": "https://...",
      "source": "Source Name",
      "guests": ["Guest 1", "Guest 2"]
    }
  ]
}
```

## üîÑ Workflow de d√©veloppement

1. **Modifier le scraper** dans `scraper/scrape.js`
2. **Tester localement** : `npm run scrape`
3. **V√©rifier les r√©sultats** : `cat data/conventions.json`
4. **Relancer l'app** : dans Expo, tirer pour rafra√Æchir
5. **Commiter** : les conventions appara√Ætront dans l'app

## üöÄ D√©ploiement

Une fois que le scraper fonctionne localement :

```bash
git add .
git commit -m "Update scraper and conventions data"
git push
```

GitHub Actions ex√©cutera le scraper automatiquement toutes les heures.

## üí° Conseils

1. **Commencer par les conventions manuelles** - Plus fiable
2. **Tester un site √† la fois** - Plus facile √† d√©bugger
3. **Regarder les logs** - Les erreurs donnent des indices
4. **√ätre patient avec le scraping** - Les sites changent souvent
5. **Respecter les robots.txt** - Certains sites l'interdisent

## üìû Besoin d'aide ?

Si le scraping ne fonctionne pas :
1. Consulter `TROUBLESHOOTING.md`
2. V√©rifier les logs du scraper
3. Utiliser les conventions manuelles en attendant

---

**Happy scraping! üî•**
